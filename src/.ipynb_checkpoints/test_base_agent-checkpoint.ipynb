{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drl4dypm.agent import *\n",
    "from drl4dypm.env import *\n",
    "import time, copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment params\n",
    "trading_days = 252\n",
    "asset_names = ['AAL','AMZN','GOOG','FB','TSLA','CVS','FDX']\n",
    "k = 10\n",
    "cost_bps = 1e-3\n",
    "path_to_data = 'data/stock_price.csv'\n",
    "\n",
    "# agent params\n",
    "num_assets = len(asset_names)\n",
    "state_dim = 3*num_assets\n",
    "action_dim = 1+num_assets\n",
    "\n",
    "critic_learning_rate = 0.1**3\n",
    "actor_learning_rate = critic_learning_rate * 0.5\n",
    "\n",
    "network_params = {\n",
    "    'actor': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 2\n",
    "        },\n",
    "        'fc': [128,64,32],\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'critic': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 2\n",
    "        },\n",
    "        'fc': [128,64,32],\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "# training params\n",
    "max_episode = 10\n",
    "min_episode_to_train = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading environment\n",
    "env = TradingEnvironment(num_steps=trading_days, \n",
    "                         asset_names=asset_names, \n",
    "                         k=k, \n",
    "                         cost_bps=cost_bps,\n",
    "                         agent_names=['base'],\n",
    "                         path_to_data=path_to_data\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-07 08:13:55,894] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# agent\n",
    "agent = RLAgent(state_dim,\n",
    "                action_dim,\n",
    "                k,\n",
    "                network_params,\n",
    "                actor_learning_rate,\n",
    "                critic_learning_rate,\n",
    "                replay_capacity=int(1e3)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-07 08:15:25,754] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode     |reward      |reward_sm   |critic_loss |actor_loss  |elp         |elp_sum     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-07 08:15:28,393] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-07 08:15:28,404] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-07 08:15:28,414] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           |9.9145      |9.9145      |0.0010      |-0.1437     |2.7792      |2.7792      \n",
      "1           |9.9166      |9.9156      |0.0003      |-0.1204     |2.6235      |5.4027      \n",
      "2           |9.8152      |9.8786      |0.0002      |-0.1012     |2.6453      |8.0480      \n",
      "3           |9.8457      |9.8690      |0.0003      |-0.1003     |2.6152      |10.6632     \n",
      "4           |9.8618      |9.8672      |0.0002      |-0.1049     |2.6152      |13.2784     \n",
      "5           |9.4830      |9.7852      |0.0003      |-0.1123     |2.6257      |15.9041     \n",
      "6           |10.0350     |9.8331      |0.0001      |-0.1176     |2.6221      |18.5262     \n",
      "7           |10.0262     |9.8670      |0.0003      |-0.1235     |2.6349      |21.1610     \n",
      "8           |9.9147      |9.8748      |0.0001      |-0.1262     |2.7098      |23.8708     \n",
      "9           |9.8325      |9.8683      |0.0000      |-0.1290     |2.6881      |26.5589     \n"
     ]
    }
   ],
   "source": [
    "reward_sm = 0\n",
    "critic_loss = []\n",
    "actor_loss = []\n",
    "\n",
    "actor_loss_i = np.inf\n",
    "critic_loss_i = np.inf\n",
    "\n",
    "elp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "cols = ['episode','reward','reward_sm','critic_loss','actor_loss','elp','elp_sum']\n",
    "line = '|'.join([f'{col:<12}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "for e in range(max_episode):\n",
    "    state, end = env.init_step()\n",
    "    last_actions = env.simulator.last_actions\n",
    "    \n",
    "    while not end:\n",
    "        with torch.no_grad():\n",
    "            # generate action\n",
    "            action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "        \n",
    "            # execute action and move to next step\n",
    "            actions = {'base': action.numpy().reshape(-1)}\n",
    "            rewards, next_state, end = env.take_step(actions, state[0])\n",
    "            \n",
    "            # store experience\n",
    "            agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': torch.tensor(actions['base'], dtype=torch.float32).view(1,-1)},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': rewards['base'],\n",
    "                'terminal': end\n",
    "            })\n",
    "            \n",
    "            \n",
    "        state = next_state\n",
    "        last_actions = actions\n",
    "        \n",
    "    # update ddpg\n",
    "    if e > min_episode_to_train:\n",
    "        actor_loss_i, critic_loss_i = agent.update(return_loss=True)\n",
    "        \n",
    "        \n",
    "    actor_loss.append(actor_loss_i)\n",
    "    critic_loss.append(critic_loss_i)\n",
    "    \n",
    "    rewards = env.get_total_rewards()\n",
    "    reward_sm = 0.9*reward_sm + 0.1*rewards['base']\n",
    "    reward_corr = reward_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if e%1 == 0:\n",
    "        elp_episode = time.time()-start_time\n",
    "        elp += elp_episode\n",
    "        start_time = time.time()\n",
    "        \n",
    "        line = f'{e:<12}|' + '|'.join([f'{col:<12.4f}' for col in [rewards['base'], reward_corr, \n",
    "                                                              critic_loss[-1], actor_loss[-1], \n",
    "                                                              elp_episode, elp]])\n",
    "        print(line)\n",
    "    \n",
    "    \n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
