{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drl4dypm.agent import *\n",
    "from drl4dypm.env import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = DataSource(num_steps=252, asset_names=['AAPL','BC'], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "path_to_data = 'data/assets.h5'\n",
    "data_source.load_data(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4585"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_source.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2000-01-03</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.596463</td>\n",
       "      <td>3.267146</td>\n",
       "      <td>3.614454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>17.012791</td>\n",
       "      <td>16.818935</td>\n",
       "      <td>17.129104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2000-01-04</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.293170</td>\n",
       "      <td>3.251081</td>\n",
       "      <td>3.554053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>16.237367</td>\n",
       "      <td>16.190842</td>\n",
       "      <td>17.152367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.341362</td>\n",
       "      <td>3.309234</td>\n",
       "      <td>3.552125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close        low       high\n",
       "date       ticker                                 \n",
       "2000-01-03 AAPL     3.596463   3.267146   3.614454\n",
       "           BC      17.012791  16.818935  17.129104\n",
       "2000-01-04 AAPL     3.293170   3.251081   3.554053\n",
       "           BC      16.237367  16.190842  17.152367\n",
       "2000-01-05 AAPL     3.341362   3.309234   3.552125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2000-01-14</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.226985</td>\n",
       "      <td>3.192607</td>\n",
       "      <td>3.285138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>16.524274</td>\n",
       "      <td>16.330418</td>\n",
       "      <td>16.718130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2000-01-18</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.339435</td>\n",
       "      <td>3.226985</td>\n",
       "      <td>3.405619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>16.625079</td>\n",
       "      <td>15.803130</td>\n",
       "      <td>16.671604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-19</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>3.423611</td>\n",
       "      <td>3.321121</td>\n",
       "      <td>3.493973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close        low       high\n",
       "date       ticker                                 \n",
       "2000-01-14 AAPL     3.226985   3.192607   3.285138\n",
       "           BC      16.524274  16.330418  16.718130\n",
       "2000-01-18 AAPL     3.339435   3.226985   3.405619\n",
       "           BC      16.625079  15.803130  16.671604\n",
       "2000-01-19 AAPL     3.423611   3.321121   3.493973"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source.offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, state, feat), end = data_source.take_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03813952, -0.01159555,  0.07427025,  0.00285714,  0.03544303,\n",
       "       -0.01462523])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.22698497, 16.52427396])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.11449587, 1.02956352, 1.        , 1.01244493,\n",
       "        1.01783194, 1.        , 1.12007134, 1.03660247],\n",
       "       [1.        , 1.02050944, 0.9826372 , 1.        , 1.00746683,\n",
       "        0.97982162, 1.        , 1.1013537 , 1.03801026],\n",
       "       [1.        , 1.03544373, 0.98826836, 1.        , 1.02548754,\n",
       "        0.97090562, 1.        , 1.10075633, 1.0145471 ],\n",
       "       [1.        , 0.94583802, 0.99718436, 1.        , 0.94583802,\n",
       "        0.97982162, 1.        , 1.06531229, 0.99999994],\n",
       "       [1.        , 0.99064087, 1.00328478, 1.        , 0.95081611,\n",
       "        0.9826372 , 1.        , 1.00557516, 1.0145471 ],\n",
       "       [1.        , 0.97321754, 1.00610036, 1.        , 0.94334897,\n",
       "        0.99718436, 1.        , 1.01802039, 1.02064752],\n",
       "       [1.        , 0.92343659, 1.00610036, 1.        , 0.90103516,\n",
       "        1.00328478, 1.        , 0.98934657, 1.02674794],\n",
       "       [1.        , 0.86808018, 1.00328478, 1.        , 0.86121041,\n",
       "        0.98545278, 1.        , 0.95081611, 1.02956352],\n",
       "       [1.        , 0.96326135, 1.01173152, 1.        , 0.92094754,\n",
       "        0.98545278, 1.        , 0.98317373, 1.02674794],\n",
       "       [1.        , 0.99999969, 0.99999994, 1.        , 0.98934657,\n",
       "        0.98826836, 1.        , 1.01802039, 1.01173152]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = TradingSimulator(num_assets=2, \n",
    "                             cost_bps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010005003335835344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.ones(2) * 0.5\n",
    "prices = np.ones(2)\n",
    "simulator.take_step(action, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.ones(2) * 0.5\n",
    "prices = np.ones(2)\n",
    "simulator.take_step(action, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark (CRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_days = 252\n",
    "asset_names = ['AAPL','BC']\n",
    "k = 10\n",
    "cost_bps = 1e-3\n",
    "path_to_data = 'data/assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# trading environment\n",
    "env = TradingEnvironment(num_steps=trading_days, \n",
    "                         asset_names=asset_names, \n",
    "                         k=k, \n",
    "                         cost_bps=cost_bps,\n",
    "                         agent_names = ['crp'],\n",
    "                         path_to_data=path_to_data\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {'crp': np.ones(3)/3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.008016212173172324 -0.024528082445371795\n"
     ]
    }
   ],
   "source": [
    "rewards, next_state, end = env.take_step(actions, state[0])\n",
    "state = next_state\n",
    "print(rewards['crp'], env.get_total_rewards()['crp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# env.data_source.offset = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 0.08368937840664353\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "state, end = env.init_step()\n",
    "\n",
    "while not end:\n",
    "    rewards, next_state, end = env.take_step(actions, state[0])\n",
    "    state = next_state\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "    \n",
    "print(step, env.get_total_rewards()['crp'])\n",
    "\n",
    "env.reset()\n",
    "# env.data_source.offset = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, in_series, out_series):\n",
    "    # self-def learn function\n",
    "    \n",
    "    predictions = list()\n",
    "    \n",
    "    n = len(in_series)\n",
    "    for i in range(n):\n",
    "        prediction = self.predict_next()\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        self.learn_one_step(out_series[i])\n",
    "        self._update_state(in_series[i])\n",
    "    \n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y,y_hat):\n",
    "    return np.sqrt(np.sum(np.mean(np.square(y-y_hat), axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drl4dypm.pydybm.base.generator import NoisySin\n",
    "from drl4dypm.pydybm.base.sgd import RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sin_series(sin_series, num_steps=252):\n",
    "    in_series = np.zeros((num_steps,1))\n",
    "    out_series = np.zeros((num_steps,1))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        in_series[t] = sin_series.next()\n",
    "    \n",
    "    out_series[:-1] = in_series[1:]\n",
    "    out_series[-1] = sin_series.next()\n",
    "    \n",
    "    return in_series, out_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_series = NoisySin(600, 80, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_series, out_series = generate_sin_series(sin_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter      |RMSE      \n",
      "0         |0.4997    \n",
      "1         |0.2296    \n",
      "2         |0.1905    \n",
      "3         |0.1543    \n",
      "4         |0.1383    \n",
      "5         |0.1393    \n",
      "6         |0.1406    \n",
      "7         |0.1416    \n",
      "8         |0.1424    \n",
      "9         |0.1428    \n"
     ]
    }
   ],
   "source": [
    "max_iter = 10\n",
    "\n",
    "\n",
    "ipm_sin = RNNGaussianDyBM(1, 1, 100, \n",
    "                          spectral_radius=0.95, sparsity=0.1,\n",
    "                         leak=1.0, random_seed=2, SGD=RMSProp())\n",
    "\n",
    "\n",
    "line = '|'.join([f'{col:<10}' for col in ['iter','RMSE']])\n",
    "print(line)\n",
    "\n",
    "# sin_series = NoisySin(600, 80, 0.1, 1)\n",
    "# in_series, out_series = generate_sin_series(sin_series)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    predictions = learn(ipm_sin, in_series, out_series)\n",
    "    \n",
    "    error = RMSE(out_series, predictions)\n",
    "    \n",
    "    line = f'{i:<10}|{error:<10.4f}'\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn on stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(data_source, feat_dim, num_steps=252):\n",
    "    data_source.reset()\n",
    "    \n",
    "    in_series = np.zeros((num_steps, feat_dim))\n",
    "    out_series = np.zeros((num_steps, feat_dim))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        (__, __, feat), __ = data_source.take_step()\n",
    "        in_series[t] = feat\n",
    "    \n",
    "    out_series[:-1] = in_series[1:]\n",
    "    \n",
    "    (__, __, feat), __ = data_source.take_step()\n",
    "    out_series[-1] = feat\n",
    "    \n",
    "    return in_series, out_series\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# init data source\n",
    "data_source = DataSource(num_steps=252, asset_names=['AAPL','BC'], k=10)\n",
    "\n",
    "path_to_data = 'data/assets.h5'\n",
    "data_source.load_data(path_to_data)\n",
    "\n",
    "data_source.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init IPM\n",
    "num_assets = 2\n",
    "feat_dim = 3*num_assets\n",
    "rnn_dim = 20\n",
    "ipm = RNNGaussianDyBM(feat_dim, feat_dim,\n",
    "                      rnn_dim, spectral_radius=0.95, sparsity=0.1,\n",
    "                     leak=1.0, SGD=RMSProp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter      |RMSE      \n",
      "0         |0.0489    \n",
      "1         |0.0487    \n",
      "2         |0.0487    \n",
      "3         |0.0486    \n",
      "4         |0.0486    \n",
      "5         |0.0486    \n",
      "6         |0.0485    \n",
      "7         |0.0485    \n",
      "8         |0.0485    \n",
      "9         |0.0485    \n"
     ]
    }
   ],
   "source": [
    "max_iter = 10\n",
    "\n",
    "line = '|'.join([f'{col:<10}' for col in ['iter','RMSE']])\n",
    "print(line)\n",
    "\n",
    "\n",
    "in_series, out_series = generate_series(data_source, feat_dim)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    predictions = learn(ipm, in_series, out_series)\n",
    "    \n",
    "    error = RMSE(out_series, predictions)\n",
    "    \n",
    "    line = f'{i:<10}|{error:<10.4f}'\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, Bounds, LinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(x):\n",
    "    u = np.array([1,1.01,0.98])\n",
    "    c = 1e-3\n",
    "    \n",
    "    return -np.dot(u,x[:3]) + c*np.sum(x[3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333, 0.06666667, 0.03333333,\n",
       "       0.03333333])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_end = [0.4,0.3,0.3]\n",
    "w0 = np.ones(3)/3\n",
    "z0 = np.abs(w_0-w_end)\n",
    "\n",
    "x0 = np.append(w_0, z_0)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = Bounds(np.zeros(6), np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., -1., -0., -0.],\n",
       "       [ 0.,  1.,  0., -0., -1., -0.],\n",
       "       [ 0.,  0.,  1., -0., -0., -1.],\n",
       "       [ 1.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx = np.block([\n",
    "    [np.eye(3),-1*np.eye(3)],\n",
    "    [np.eye(3),np.eye(3)],\n",
    "    [np.ones(3), np.zeros(3)]\n",
    "])\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1. , -1. , -1. ,  0.4,  0.3,  0.3,  1. ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_bnd = np.concatenate([-1*np.ones(3), w_end, [1]])\n",
    "left_bnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4, 0.3, 0.3, 2. , 2. , 2. , 1. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_bnd = np.concatenate([w_end, 2*np.ones(3), [1]])\n",
    "right_bnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_constr = LinearConstraint(mtx, left_bnd, right_bnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(obj, x0, method='trust-constr', constraints=[lin_constr], bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.12974443e-04, 9.98686245e-01, 4.00781041e-04, 4.04899671e-01,\n",
       "       7.00793884e-01, 3.00763346e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = res.x[:3]\n",
    "z_star = res.x[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39908703, 0.69868624, 0.29959922])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(w_star-w_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40489967, 0.70079388, 0.30076335])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment params\n",
    "trading_days = 252\n",
    "asset_names = ['AAPL','BC']\n",
    "k = 10\n",
    "cost_bps = 1e-3\n",
    "path_to_data = 'data/assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent params\n",
    "num_assets = len(asset_names)\n",
    "state_dim = 3*(1+num_assets)\n",
    "action_dim = 1+num_assets\n",
    "\n",
    "critic_learning_rate = 0.1**3\n",
    "actor_learning_rate = critic_learning_rate * 0.01\n",
    "\n",
    "network_params = {\n",
    "    'actor': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'critic': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "max_episode = 10\n",
    "min_episode_to_train = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initiate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# trading environment\n",
    "env = TradingEnvironment(num_steps=trading_days, \n",
    "                         asset_names=asset_names, \n",
    "                         k=k, \n",
    "                         cost_bps=cost_bps,\n",
    "                         path_to_data=path_to_data\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 15:58:36,920] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# agent\n",
    "agent = BaseAgent(state_dim,\n",
    "                  action_dim,\n",
    "                  network_params,\n",
    "                  actor_learning_rate,\n",
    "                  critic_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3297, 0.3768, 0.4520]])\n",
      "0.07360835403823848\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "    print(action)\n",
    "    \n",
    "    reward, next_state, end = env.take_step(action.numpy().reshape(-1), state[0])\n",
    "    print(reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.store_transition({\n",
    "        'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "        'action': {'action': action},\n",
    "        'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "        'reward': reward,\n",
    "        'terminal': end\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.data_source.step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_crp = TradingSimulator(num_assets, cost_bps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 15:58:45,026] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode         |reward          |reward_sm       |reward_crp      |reward_crp_sm   |elp             |elp_sum         \n",
      "0               |3.9137          |3.9137          |0.1681          |0.1681          |0.5794          |0.5794          \n",
      "1               |4.0311          |3.9755          |0.3618          |0.2700          |0.5696          |1.1490          \n",
      "2               |4.0587          |4.0062          |0.1998          |0.2441          |0.5663          |1.7154          \n",
      "3               |3.7491          |3.9314          |0.0339          |0.1830          |0.5235          |2.2389          \n",
      "4               |3.7937          |3.8978          |0.0311          |0.1459          |0.5297          |2.7686          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 15:58:48,319] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-02-28 15:58:48,326] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-02-28 15:58:48,333] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5               |3.5946          |3.8331          |-0.0196         |0.1106          |0.5242          |3.2928          \n",
      "6               |3.8580          |3.8379          |0.2161          |0.1308          |16.0240         |19.3169         \n",
      "7               |3.8685          |3.8433          |0.2071          |0.1442          |16.4764         |35.7933         \n",
      "8               |3.9687          |3.8637          |0.3318          |0.1748          |16.4415         |52.2348         \n",
      "9               |4.0896          |3.8984          |0.2995          |0.1940          |16.3143         |68.5491         \n"
     ]
    }
   ],
   "source": [
    "reward_sm = 0\n",
    "reward_crp_sm = 0\n",
    "\n",
    "elp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "cols = ['episode','reward','reward_sm','reward_crp','reward_crp_sm','elp','elp_sum']\n",
    "line = '|'.join([f'{col:<16}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "for e in range(max_episode):\n",
    "    state, end = env.init_step()\n",
    "    \n",
    "    while not end:\n",
    "        with torch.no_grad():\n",
    "            # generate action by epsilon-greedy \n",
    "            action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "        \n",
    "            # execute action and move to next step\n",
    "            reward, next_state, end = env.take_step(action.numpy().reshape(-1), state[0])\n",
    "            \n",
    "            # execute CRP action\n",
    "            action_crp = np.ones(1+num_assets)/(1+num_assets)\n",
    "            sim_crp.take_step(action_crp, state[0])\n",
    "        \n",
    "            # store experience\n",
    "            agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': action},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': reward,\n",
    "                'terminal': end\n",
    "            })\n",
    "            \n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "        # update ddpg\n",
    "        if e > min_episode_to_train:\n",
    "            agent.update()\n",
    "        \n",
    "    \n",
    "    \n",
    "    reward = env.get_total_reward()\n",
    "    reward_sm = 0.9*reward_sm + 0.1*reward\n",
    "    reward_corr = reward_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    reward_crp = sim_crp.get_total_reward()\n",
    "    reward_crp_sm = 0.9*reward_crp_sm + 0.1*reward_crp\n",
    "    reward_crp_corr = reward_crp_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    elp_episode = time.time()-start_time\n",
    "    elp += elp_episode\n",
    "    line = f'{e:<16}|' + '|'.join([f'{col:<16.4f}' for col in [reward, reward_corr, \n",
    "                                                          reward_crp, reward_crp_corr,\n",
    "                                                          elp_episode, elp]])\n",
    "    print(line)\n",
    "    \n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    sim_crp.reset()\n",
    "    start_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode         |reward          |reward_sm       |reward_crp      |reward_crp_sm   |elp             |elp_sum         \n",
      "0               |34.1880         |34.1880         |0.3078          |0.3078          |0.5122          |0.5122          \n",
      "1               |34.3944         |34.2966         |0.1820          |0.2416          |0.5390          |1.0511          \n",
      "2               |34.8612         |34.5049         |0.3532          |0.2828          |0.5336          |1.5848          \n",
      "3               |35.2292         |34.7155         |0.1356          |0.2400          |0.5316          |2.1163          \n",
      "4               |35.1234         |34.8151         |0.3251          |0.2607          |0.5417          |2.6580          \n",
      "5               |33.6763         |34.5721         |-0.0299         |0.1987          |0.5373          |3.1953          \n",
      "6               |34.0263         |34.4675         |0.0933          |0.1785          |16.3121         |19.5075         \n",
      "7               |34.6552         |34.5004         |-0.1789         |0.1158          |16.2123         |35.7198         \n",
      "8               |34.8343         |34.5549         |0.3472          |0.1536          |16.3334         |52.0532         \n",
      "9               |35.1518         |34.6466         |0.4407          |0.1976          |16.3436         |68.3968         \n"
     ]
    }
   ],
   "source": [
    "# noise param (0,0.1)\n",
    "\n",
    "reward_sm = 0\n",
    "reward_crp_sm = 0\n",
    "\n",
    "elp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "cols = ['episode','reward','reward_sm','reward_crp','reward_crp_sm','elp','elp_sum']\n",
    "line = '|'.join([f'{col:<16}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "for e in range(max_episode):\n",
    "    state, end = env.init_step()\n",
    "    \n",
    "    while not end:\n",
    "        with torch.no_grad():\n",
    "            # generate action by epsilon-greedy \n",
    "            action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "        \n",
    "            # execute action and move to next step\n",
    "            reward, next_state, end = env.take_step(action.numpy().reshape(-1), state[0])\n",
    "            \n",
    "            # execute CRP action\n",
    "            action_crp = np.ones(1+num_assets)/(1+num_assets)\n",
    "            sim_crp.take_step(action_crp, state[0])\n",
    "        \n",
    "            # store experience\n",
    "            agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': action},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': reward,\n",
    "                'terminal': end\n",
    "            })\n",
    "            \n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "        # update ddpg\n",
    "        if e > min_episode_to_train:\n",
    "            agent.update()\n",
    "        \n",
    "    \n",
    "    \n",
    "    reward = env.get_total_reward()\n",
    "    reward_sm = 0.9*reward_sm + 0.1*reward\n",
    "    reward_corr = reward_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    reward_crp = sim_crp.get_total_reward()\n",
    "    reward_crp_sm = 0.9*reward_crp_sm + 0.1*reward_crp\n",
    "    reward_crp_corr = reward_crp_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    elp_episode = time.time()-start_time\n",
    "    elp += elp_episode\n",
    "    line = f'{e:<16}|' + '|'.join([f'{col:<16.4f}' for col in [reward, reward_corr, \n",
    "                                                          reward_crp, reward_crp_corr,\n",
    "                                                          elp_episode, elp]])\n",
    "    print(line)\n",
    "    \n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    sim_crp.reset()\n",
    "    start_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment params\n",
    "trading_days = 252\n",
    "asset_names = ['AAPL','BC']\n",
    "k = 10\n",
    "cost_bps = 1e-3\n",
    "path_to_data = 'data/assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent params\n",
    "num_assets = len(asset_names)\n",
    "state_dim = 3*(1+num_assets)\n",
    "action_dim = 1+num_assets\n",
    "ipm_dim = 3*num_assets\n",
    "\n",
    "critic_learning_rate = 0.1**3\n",
    "actor_learning_rate = critic_learning_rate * 0.01\n",
    "ipm_learning_rate = 0.1**3\n",
    "\n",
    "network_params = {\n",
    "    'actor': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'critic': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'ipm': {\n",
    "        'rnn_dim': 20,\n",
    "        'delay': 3,\n",
    "        'decay_rates': [0.1,0.2,0.5,0.8],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "max_episode = 10\n",
    "min_episode_to_train = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initiate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# trading environment\n",
    "env = TradingEnvironment(num_steps=trading_days, \n",
    "                         asset_names=asset_names, \n",
    "                         k=k, \n",
    "                         cost_bps=cost_bps,\n",
    "                         path_to_data=path_to_data\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 18:38:45,061] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# agent\n",
    "agent = IPMAgent(state_dim,\n",
    "                 action_dim,\n",
    "                 ipm_dim,\n",
    "                 network_params,\n",
    "                 actor_learning_rate,\n",
    "                 critic_learning_rate,\n",
    "                 ipm_learning_rate\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 17:59:52,683] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.IPMActor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3037, 0.4444, 0.3560]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.ddpg_per.act_with_noise(\n",
    "    {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "    'ipm': torch.tensor(np.zeros(6), dtype=torch.float32).view(1,-1)},\n",
    "    noise_param=agent.noise_param, mode=agent.noise_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store transition and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, next_state, end = env.take_step(np.array([0.3037, 0.4444, 0.3560]), state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00540482,  0.00715746, -0.01699502,  0.01291711, -0.00745109,\n",
       "        0.00707071])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.store_transition({\n",
    "        'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                 'ipm': torch.tensor(np.zeros(6), dtype=torch.float32).view(1,-1)},\n",
    "        'action': {'action': torch.tensor([[0.3037, 0.4444, 0.3560]])},\n",
    "        'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                      'ipm': torch.tensor(np.zeros(6), dtype=torch.float32).view(1,-1)},\n",
    "        'reward': reward,\n",
    "        'terminal': end\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-02-28 18:22:25,262] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.IPMActor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-02-28 18:22:25,311] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.IPMCritic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-02-28 18:22:25,323] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.IPMCritic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-02-28 18:22:25,371] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.IPMActor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0661206841468811, 0.0009626416433032858)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.ddpg_per.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.ipm.init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00335299, -0.00335297, -0.00335296,  0.00335299,  0.00335298,\n",
       "        0.00335297])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipm_predict = agent.ipm_predict_and_learn(state[-1],)\n",
    "ipm_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4278, 0.4215, 0.3403]])\n",
      "0.08418597467026154\n",
      "[-0.00476514 -0.00304944 -0.00380439  0.00024339 -0.00329296 -0.00100855]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                             torch.tensor(ipm_predict, dtype=torch.float32).view(1,-1))\n",
    "    print(action)\n",
    "    \n",
    "    reward, next_state, end = env.take_step(action.numpy().reshape(-1), state[0])\n",
    "    print(reward)\n",
    "    \n",
    "    next_ipm_predict = agent.ipm_predict_and_learn(state[-1], next_state[-1])\n",
    "    print(next_ipm_predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.store_transition({\n",
    "        'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                 'ipm': torch.tensor(ipm_predict, dtype=torch.float32).view(1,-1)},\n",
    "        'action': {'action': action},\n",
    "        'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                      'ipm': torch.tensor(next_ipm_predict, dtype=torch.float32).view(1,-1)},\n",
    "        'reward': reward,\n",
    "        'terminal': end\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.08418597467026154 0.7071450234957418\n"
     ]
    }
   ],
   "source": [
    "state = next_state\n",
    "ipm_predict = next_ipm_predict\n",
    "\n",
    "agent.update()\n",
    "\n",
    "print(env.data_source.step, reward, env.get_total_reward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "agent.ipm_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.noise_param=(0,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_crp = TradingSimulator(num_assets, cost_bps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode         |reward          |reward_sm       |reward_crp      |reward_crp_sm   |elp             |elp_sum         \n",
      "0               |3.9924          |3.9924          |0.2512          |0.2512          |0.6688          |0.6688          \n",
      "1               |3.6215          |3.7972          |-0.0907         |0.0713          |0.6949          |1.3637          \n",
      "2               |3.6998          |3.7613          |-0.0771         |0.0165          |0.6348          |1.9985          \n",
      "3               |3.6508          |3.7292          |0.0501          |0.0263          |0.6265          |2.6249          \n",
      "4               |3.7776          |3.7410          |0.0197          |0.0247          |0.6177          |3.2426          \n",
      "5               |3.9247          |3.7802          |0.1929          |0.0606          |0.6175          |3.8601          \n",
      "6               |3.8890          |3.8010          |0.2299          |0.0930          |17.0033         |20.8634         \n",
      "7               |3.7786          |3.7971          |0.1075          |0.0956          |17.0506         |37.9140         \n",
      "8               |3.7721          |3.7930          |0.1055          |0.0972          |17.1130         |55.0270         \n",
      "9               |4.1961          |3.8549          |0.4679          |0.1541          |17.4063         |72.4334         \n"
     ]
    }
   ],
   "source": [
    "reward_sm = 0\n",
    "reward_crp_sm = 0\n",
    "\n",
    "elp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "cols = ['episode','reward','reward_sm','reward_crp','reward_crp_sm','elp','elp_sum']\n",
    "line = '|'.join([f'{col:<16}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "for e in range(max_episode):\n",
    "    state, end = env.init_step()\n",
    "    \n",
    "    # get IPM prediction\n",
    "    ipm_predict = agent.ipm_predict_and_learn(state[-1],)\n",
    "    \n",
    "    while not end:\n",
    "        with torch.no_grad():\n",
    "            # generate action by epsilon-greedy \n",
    "            action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                                     torch.tensor(ipm_predict, dtype=torch.float32).view(1,-1))\n",
    "        \n",
    "            # execute action and move to next step\n",
    "            reward, next_state, end = env.take_step(action.numpy().reshape(-1), state[0])\n",
    "            \n",
    "            # train IPM with next state and get next prediction\n",
    "            next_ipm_predict = agent.ipm_predict_and_learn(state[-1], next_state[-1])\n",
    "            \n",
    "            # execute CRP action\n",
    "            action_crp = np.ones(1+num_assets)/(1+num_assets)\n",
    "            sim_crp.take_step(action_crp, state[0])\n",
    "        \n",
    "            # store experience\n",
    "            agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                         'ipm': torch.tensor(ipm_predict, dtype=torch.float32).view(1,-1)},\n",
    "                'action': {'action': action},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1),\n",
    "                              'ipm': torch.tensor(next_ipm_predict, dtype=torch.float32).view(1,-1)},\n",
    "                'reward': reward,\n",
    "                'terminal': end\n",
    "            })\n",
    "            \n",
    "            \n",
    "        state = next_state\n",
    "        ipm_predict = next_ipm_predict\n",
    "        \n",
    "        \n",
    "        # update ddpg\n",
    "        if e > min_episode_to_train:\n",
    "            agent.update()\n",
    "        \n",
    "    \n",
    "    \n",
    "    reward = env.get_total_reward()\n",
    "    reward_sm = 0.9*reward_sm + 0.1*reward\n",
    "    reward_corr = reward_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    reward_crp = sim_crp.get_total_reward()\n",
    "    reward_crp_sm = 0.9*reward_crp_sm + 0.1*reward_crp\n",
    "    reward_crp_corr = reward_crp_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    elp_episode = time.time()-start_time\n",
    "    elp += elp_episode\n",
    "    line = f'{e:<16}|' + '|'.join([f'{col:<16.4f}' for col in [reward, reward_corr, \n",
    "                                                          reward_crp, reward_crp_corr,\n",
    "                                                          elp_episode, elp]])\n",
    "    print(line)\n",
    "    \n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    agent.ipm_init()\n",
    "    sim_crp.reset()\n",
    "    start_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCM agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment params\n",
    "trading_days = 252\n",
    "asset_names = ['AAPL','BC']\n",
    "k = 10\n",
    "cost_bps = 1e-3\n",
    "path_to_data = 'data/assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent params\n",
    "num_assets = len(asset_names)\n",
    "state_dim = 3*(1+num_assets)\n",
    "action_dim = 1+num_assets\n",
    "bcm_update_rate = .1\n",
    "\n",
    "critic_learning_rate = 0.1**3\n",
    "actor_learning_rate = critic_learning_rate * 0.01\n",
    "\n",
    "network_params = {\n",
    "    'actor': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'critic': {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "max_episode = 10\n",
    "min_episode_to_train = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initiate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/assets.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# trading environment\n",
    "env = TradingEnvironment(num_steps=trading_days, \n",
    "                         asset_names=asset_names, \n",
    "                         k=k, \n",
    "                         cost_bps=cost_bps,\n",
    "                         agent_names = ['bcm','crp'],\n",
    "                         path_to_data=path_to_data\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 11:47:51,181] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# RL agent\n",
    "agent = BCMAgent(state_dim,\n",
    "                  action_dim,\n",
    "                 cost_bps,\n",
    "                 bcm_update_rate,\n",
    "                  network_params,\n",
    "                  actor_learning_rate,\n",
    "                  critic_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark rule-based agent\n",
    "agent_crp = CRPAgent(action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get BCM action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 11:11:31,353] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3081, 0.3841, 0.3268]])\n",
      "{'bcm': -0.0014038208010464569}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "    print(action)\n",
    "    \n",
    "    actions = {'bcm': action.numpy().reshape(-1)}\n",
    "    rewards, next_state, end = env.take_step(actions, state[0])\n",
    "    print(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.         103.66241648  46.66306948] [  1.         103.08932029  45.09883501]\n"
     ]
    }
   ],
   "source": [
    "prices = state[0]\n",
    "next_prices = next_state[0]\n",
    "print(prices, next_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99484924e-01, 2.30203370e-04, 2.84872551e-04])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcm_action = agent.get_bcm_action(prices, next_prices)\n",
    "bcm_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store transition and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': torch.tensor(actions['bcm'], dtype=torch.float32).view(1,-1)},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': rewards['bcm'],\n",
    "                'terminal': False,\n",
    "                'bcm_action': torch.tensor(bcm_action)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 11:11:44,542] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 11:11:44,558] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 11:11:44,569] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0741, grad_fn=<NegBackward>)\n",
      "tensor(0.7063, dtype=torch.float64, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "agent.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, end = env.init_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.3404, 0.3809]])\n",
      "{'bcm': 0.010843541529001528}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "    print(action)\n",
    "    \n",
    "    actions = {'bcm': action.numpy().reshape(-1)}\n",
    "    rewards, next_state, end = env.take_step(actions, state[0])\n",
    "    print(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.57962674e-04 9.99366369e-01 7.56685610e-05]\n",
      "5 0.010843541529001528 0.04247539380764243\n"
     ]
    }
   ],
   "source": [
    "bcm_action = agent.get_bcm_action(state[0], next_state[0])\n",
    "print(bcm_action)\n",
    "\n",
    "agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': torch.tensor(actions['bcm'], dtype=torch.float32).view(1,-1)},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': rewards['bcm'],\n",
    "                'terminal': False,\n",
    "                'bcm_action': torch.tensor(bcm_action)\n",
    "            })\n",
    "\n",
    "agent.update()\n",
    "\n",
    "print(env.data_source.step, rewards['bcm'], env.get_total_rewards()['bcm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 11:48:08,891] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode         |reward          |reward_sm       |reward_crp      |reward_crp_sm   |elp             |elp_sum         \n",
      "0               |3.9160          |3.9160          |0.1212          |0.1212          |5.2540          |5.2540          \n",
      "1               |3.4089          |3.6491          |-0.5495         |-0.2318         |4.9770          |10.2310         \n",
      "2               |3.9362          |3.7551          |0.1749          |-0.0817         |4.9593          |15.1903         \n",
      "3               |3.6652          |3.7289          |-0.0352         |-0.0682         |4.9814          |20.1716         \n",
      "4               |4.0571          |3.8091          |0.2724          |0.0150          |5.0156          |25.1872         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 11:48:39,116] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 11:48:39,125] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 11:48:39,133] <WARNING>:default_logger:You have not specified the i/o device of your model <class 'drl4dypm.agent.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5               |4.1861          |3.8895          |0.3973          |0.0966          |5.0161          |30.2034         \n",
      "6               |3.3478          |3.7857          |-0.2316         |0.0337          |20.2810         |50.4844         \n",
      "7               |4.1871          |3.8562          |0.4399          |0.1050          |20.1027         |70.5871         \n",
      "8               |4.0136          |3.8819          |0.2141          |0.1228          |20.0825         |90.6696         \n",
      "9               |3.8229          |3.8728          |0.1257          |0.1233          |20.1986         |110.8683        \n"
     ]
    }
   ],
   "source": [
    "reward_sm = 0\n",
    "reward_crp_sm = 0\n",
    "\n",
    "elp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "cols = ['episode','reward','reward_sm','reward_crp','reward_crp_sm','elp','elp_sum']\n",
    "line = '|'.join([f'{col:<16}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "\n",
    "for e in range(max_episode):\n",
    "    state, end = env.init_step()\n",
    "    \n",
    "    while not end:\n",
    "        with torch.no_grad():\n",
    "            # generate action by epsilon-greedy \n",
    "            action = agent.get_action(torch.tensor(state[1], dtype=torch.float32).view(1,k,-1))\n",
    "            action_crp = agent_crp.get_action()\n",
    "            \n",
    "            # execute action and move to next step\n",
    "            actions = {'bcm': action.numpy().reshape(-1), 'crp': action_crp}\n",
    "            rewards, next_state, end = env.take_step(actions, state[0])\n",
    "            \n",
    "            # get CBM action\n",
    "            bcm_action = agent.get_bcm_action(state[0], next_state[0])\n",
    "            \n",
    "        \n",
    "            # store experience\n",
    "            agent.store_transition({\n",
    "                'state': {'state': torch.tensor(state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'action': {'action': torch.tensor(actions['bcm'], dtype=torch.float32).view(1,-1)},\n",
    "                'next_state': {'state': torch.tensor(next_state[1], dtype=torch.float32).view(1,k,-1)},\n",
    "                'reward': rewards['bcm'],\n",
    "                'terminal': False,\n",
    "                'bcm_action': torch.tensor(bcm_action)\n",
    "            })\n",
    "            \n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        # update ddpg\n",
    "        if e > min_episode_to_train:\n",
    "            agent.update()\n",
    "        \n",
    "    \n",
    "    \n",
    "    rewards = env.get_total_rewards()\n",
    "    reward_sm = 0.9*reward_sm + 0.1*rewards['bcm']\n",
    "    reward_corr = reward_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    reward_crp_sm = 0.9*reward_crp_sm + 0.1*rewards['crp']\n",
    "    reward_crp_corr = reward_crp_sm/(1-0.9**(e+1))\n",
    "    \n",
    "    elp_episode = time.time()-start_time\n",
    "    elp += elp_episode\n",
    "    line = f'{e:<16}|' + '|'.join([f'{col:<16.4f}' for col in [rewards['bcm'], reward_corr, \n",
    "                                                          rewards['crp'], reward_crp_corr,\n",
    "                                                          elp_episode, elp]])\n",
    "    print(line)\n",
    "    \n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    start_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "- Differences from original paper\n",
    "    - action is generated from by act_with_noise, not a perturbed actor network\n",
    "    - last action is not added as inputs of actor network\n",
    "    - only last-step output of LSTM is used as input of following layers\n",
    "    - market index performance is not added to state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    "        'lstm': {\n",
    "            'hidden_dim': 20,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'fc': [64,32],\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "\n",
    "num_assets = 2\n",
    "state_dim = 3*(1+num_assets)\n",
    "action_dim = 1+num_assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_dim, action_dim, net_params)\n",
    "critic =Critic(state_dim, action_dim, net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_target = Actor(state_dim, action_dim, net_params)\n",
    "critic_target =Critic(state_dim, action_dim, net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = lambda params, lr: torch.optim.Adam(params, lr=lr, weight_decay=1e-6)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_per = DDPGPer(actor, actor_target,\n",
    "                 critic, critic_target,\n",
    "                 optimizer=optimizer,\n",
    "                 criterion=criterion,\n",
    "                 batch_size=128,\n",
    "                 actor_learning_rate=1e-3,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 discount=0.99,\n",
    "                 replay_size=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.normal(0,1,size=(1,k,state_dim))\n",
    "next_state = torch.normal(0,1,size=(1,k,state_dim))\n",
    "\n",
    "temp = torch.normal(0,1, size=(1,action_dim))\n",
    "action = temp/temp.sum()\n",
    "\n",
    "# bcm_action = torch.ones(1,action_dim)/action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcm_action = torch.Tensor([[0.2,0.3,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = {\n",
    "                'state': {'state': state},\n",
    "                'action': {'action': action},\n",
    "                'next_state': {'state': next_state},\n",
    "                'reward': 0.0,\n",
    "                'terminal': False,\n",
    "                'bcm_action': bcm_action\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_per.store_transition(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, (state, action, reward, next_state, bcm_action, terminal, others), index, is_weight = \\\n",
    "\t\tddpg_per.replay_buffer.sample_batch(10, True,\n",
    "\t\t\t\t\t\t\t\t\t   sample_attrs=['state','action','reward','next_state','bcm_action','terminal','*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3333, 0.3333, 0.3333]]),\n",
       " tensor([[0.3333, 0.3333, 0.3333]]),\n",
       " tensor([[0.3333, 0.3333, 0.3333]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]]),\n",
       " tensor([[0.2000, 0.3000, 0.5000]])]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcm_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcm_action = torch.stack(bcm_action).view(-1,action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3069, 0.3160, 0.3771],\n",
       "        [0.3195, 0.3083, 0.3723],\n",
       "        [0.3081, 0.3126, 0.3793],\n",
       "        [0.3087, 0.3172, 0.3741],\n",
       "        [0.3109, 0.3178, 0.3713],\n",
       "        [0.3118, 0.3196, 0.3686],\n",
       "        [0.3090, 0.3160, 0.3750],\n",
       "        [0.3123, 0.3135, 0.3743],\n",
       "        [0.3112, 0.3191, 0.3697],\n",
       "        [0.3064, 0.3164, 0.3773]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_action = ddpg_per.action_transform_function(\n",
    "\t\t\tddpg_per._act(state), state, others\n",
    "\t\t\t)\n",
    "cur_action['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_action = cur_action['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6277, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = bcm_action * torch.log(cur_action+eps) + (1-bcm_action)*torch.log(1-cur_action+eps)\n",
    "temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(np.arange(6).reshape(2,3), dtype=torch.float)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 2.],\n",
       "        [4., 4., 5.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(np.arange(6).reshape(2,3), dtype=torch.float)\n",
    "b[0,0] += 2\n",
    "b[1,0] += 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2361)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.square(a-b, p=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
