{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machin.frame.algorithms import DDPGPer\n",
    "from machin.utils.logging import default_logger as logger\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "\n",
    "from drl4dypm.env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "observe_dim = 3\n",
    "action_dim = 1\n",
    "action_range = 2\n",
    "max_episodes = 100\n",
    "max_steps = 200\n",
    "noise_param = (0, 0.2)\n",
    "noise_mode = \"normal\"\n",
    "solved_reward = -150\n",
    "solved_repeat = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_range):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, action_dim)\n",
    "        self.action_range = action_range\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = t.relu(self.fc1(state))\n",
    "        a = t.relu(self.fc2(a))\n",
    "        a = t.tanh(self.fc3(a)) * self.action_range\n",
    "        return a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = t.cat([state, action], 1)\n",
    "        q = t.relu(self.fc1(state_action))\n",
    "        q = t.relu(self.fc2(q))\n",
    "        q = self.fc3(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 20:43:57,150] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "actor = Actor(observe_dim, action_dim, action_range)\n",
    "actor_t = Actor(observe_dim, action_dim, action_range)\n",
    "critic = Critic(observe_dim, action_dim)\n",
    "critic_t = Critic(observe_dim, action_dim)\n",
    "\n",
    "ddpg_per = DDPGPer(actor, actor_t, critic, critic_t,\n",
    "                   t.optim.Adam,\n",
    "                   nn.MSELoss(reduction='sum'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, step, reward_fulfilled = 0, 0, 0\n",
    "smoothed_total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2021-03-06 20:48:57,627] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 20:48:57,635] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
      "\u001b[33m[2021-03-06 20:48:57,637] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
      "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode     |reward      |critic_loss |actor_loss  \n",
      "2           |-281.1770   |5.7275      |-3.2711     \n",
      "3           |-376.9814   |1.2157      |-8.4578     \n",
      "4           |-415.6200   |0.1798      |-13.3615    \n",
      "5           |-518.8913   |0.7134      |-19.1177    \n",
      "6           |-622.6979   |0.3976      |-24.6760    \n",
      "7           |-715.9733   |1.3380      |-31.8608    \n",
      "8           |-799.7304   |2.0065      |-35.4326    \n",
      "9           |-875.5328   |1.8675      |-41.7894    \n",
      "10          |-932.5437   |2.8006      |-48.7506    \n",
      "11          |-949.0209   |1.1675      |-53.2349    \n",
      "12          |-980.1068   |3.5369      |-58.1922    \n",
      "13          |-983.2745   |3.1872      |-65.7600    \n",
      "14          |-989.6856   |2.7915      |-69.6690    \n",
      "15          |-980.1767   |5.6098      |-70.2010    \n",
      "16          |-986.2443   |11.5356     |-72.8416    \n",
      "17          |-1031.0578  |4.2731      |-70.2140    \n",
      "18          |-1042.6977  |1.3655      |-71.8187    \n",
      "19          |-1043.5948  |1.6224      |-84.8745    \n",
      "20          |-966.4019   |8.1501      |-82.8143    \n",
      "21          |-883.6534   |3.9772      |-82.7222    \n",
      "22          |-821.4805   |14.4018     |-87.2876    \n",
      "23          |-778.9259   |3.8048      |-91.4560    \n",
      "24          |-749.3069   |12.3560     |-91.8296    \n",
      "25          |-748.4398   |15.2707     |-98.2990    \n",
      "26          |-748.7134   |11.5824     |-99.3276    \n",
      "27          |-754.0203   |17.3925     |-98.1962    \n",
      "28          |-743.8082   |11.2105     |-102.4907   \n",
      "29          |-670.2612   |6.4219      |-100.3697   \n",
      "30          |-603.8205   |2.2606      |-103.0871   \n",
      "31          |-648.5825   |14.6438     |-107.1919   \n",
      "32          |-691.0313   |11.8756     |-113.5506   \n",
      "33          |-622.1677   |5.5727      |-111.6065   \n",
      "34          |-659.2238   |24.8983     |-118.2378   \n",
      "35          |-693.4098   |8.6757      |-120.2220   \n",
      "36          |-625.2361   |6.8106      |-118.8123   \n",
      "37          |-601.9800   |3.6687      |-118.5595   \n",
      "38          |-543.2264   |14.6540     |-122.2535   \n",
      "39          |-526.1283   |7.5807      |-117.7105   \n",
      "40          |-580.0252   |6.3876      |-128.0557   \n",
      "41          |-639.3698   |3.7557      |-123.5082   \n",
      "42          |-723.7417   |17.8354     |-134.1647   \n",
      "43          |-783.6238   |17.8745     |-136.4809   \n",
      "44          |-849.9014   |21.8187     |-134.5596   \n",
      "45          |-901.1769   |7.8361      |-132.3177   \n",
      "46          |-943.7205   |8.7877      |-139.5248   \n",
      "47          |-991.8060   |6.8298      |-139.8165   \n",
      "48          |-1033.3231  |20.1214     |-144.1469   \n",
      "49          |-1068.4188  |14.0042     |-141.3796   \n",
      "50          |-1092.4037  |21.6322     |-145.3175   \n",
      "51          |-1123.5333  |5.2473      |-146.4000   \n",
      "52          |-1150.1468  |32.8660     |-146.6455   \n",
      "53          |-1169.3654  |10.1143     |-156.0186   \n",
      "54          |-1180.0675  |3.9963      |-154.5461   \n",
      "55          |-1196.3054  |6.4594      |-158.4329   \n",
      "56          |-1195.1781  |34.6821     |-159.6155   \n",
      "57          |-1190.2764  |17.4929     |-164.7156   \n",
      "58          |-1180.5640  |39.0602     |-164.8230   \n",
      "59          |-1075.6312  |20.3527     |-160.1850   \n",
      "60          |-992.9572   |22.1697     |-160.9074   \n",
      "61          |-894.7227   |40.0390     |-163.4486   \n",
      "62          |-830.5676   |29.9317     |-170.5225   \n",
      "63          |-760.7468   |4.0071      |-160.9241   \n",
      "64          |-685.7508   |6.2981      |-153.5072   \n",
      "65          |-669.4906   |33.2547     |-153.8272   \n",
      "66          |-631.3381   |4.3084      |-152.4889   \n",
      "67          |-621.6945   |7.0753      |-166.3764   \n",
      "68          |-583.9761   |20.9326     |-158.4858   \n",
      "69          |-526.6749   |6.3763      |-157.6801   \n",
      "70          |-487.0431   |35.2183     |-157.3997   \n",
      "71          |-472.5222   |3.2996      |-151.3250   \n",
      "72          |-449.8424   |32.6787     |-147.8575   \n",
      "73          |-428.3017   |21.5118     |-152.1138   \n",
      "74          |-398.8177   |5.5585      |-144.8269   \n",
      "75          |-371.7058   |26.0229     |-150.9578   \n",
      "76          |-370.5643   |3.4090      |-145.1873   \n",
      "77          |-346.2801   |2.6638      |-134.8961   \n",
      "78          |-346.9835   |36.3915     |-147.1253   \n",
      "79          |-325.1996   |13.6439     |-140.7808   \n",
      "80          |-305.4038   |10.7954     |-129.4840   \n",
      "81          |-298.4124   |12.0950     |-144.6466   \n",
      "82          |-292.7008   |12.3703     |-134.6540   \n",
      "83          |-298.9035   |9.7374      |-142.1958   \n",
      "84          |-293.3110   |4.5478      |-142.9535   \n",
      "85          |-288.0201   |6.4870      |-139.2222   \n",
      "86          |-293.4033   |38.8469     |-134.8294   \n",
      "87          |-287.6721   |11.0540     |-130.1808   \n",
      "88          |-282.7990   |11.4338     |-113.2643   \n",
      "89          |-290.0802   |20.0245     |-120.5375   \n",
      "90          |-285.6079   |25.7489     |-114.9136   \n",
      "91          |-280.0658   |20.1472     |-111.5664   \n",
      "92          |-263.7839   |16.0755     |-99.6071    \n",
      "93          |-272.8173   |7.1084      |-96.9797    \n",
      "94          |-258.1123   |20.8841     |-84.1905    \n",
      "95          |-267.5879   |7.6520      |-94.9727    \n",
      "96          |-276.5227   |22.2899     |-79.3990    \n",
      "97          |-261.1112   |2.7642      |-76.8780    \n",
      "98          |-246.9776   |43.8081     |-81.7689    \n",
      "99          |-222.5958   |50.6821     |-72.3714    \n",
      "100         |-212.0327   |5.1878      |-73.8265    \n"
     ]
    }
   ],
   "source": [
    "cols = ['episode','reward','critic_loss','actor_loss']\n",
    "line = '|'.join([f'{col:<12}' for col in cols])\n",
    "print(line)\n",
    "\n",
    "\n",
    "while episode < max_episodes:\n",
    "    episode += 1\n",
    "    total_reward = 0\n",
    "    terminal = False\n",
    "    step = 0\n",
    "    state = t.tensor(env.reset(), dtype=t.float32).view(1, observe_dim)\n",
    "\n",
    "    while not terminal and step <= max_steps:\n",
    "        step += 1\n",
    "        with t.no_grad():\n",
    "            old_state = state\n",
    "            # agent model inference\n",
    "            action = ddpg_per.act_with_noise(\n",
    "                        {\"state\": old_state},\n",
    "                        noise_param=noise_param,\n",
    "                        mode=noise_mode\n",
    "                    )\n",
    "            state, reward, terminal, _ = env.step(action.numpy())\n",
    "            state = t.tensor(state, dtype=t.float32).view(1, observe_dim)\n",
    "            total_reward += reward[0]\n",
    "\n",
    "            ddpg_per.store_transition({\n",
    "                \"state\": {\"state\": old_state},\n",
    "                \"action\": {\"action\": action},\n",
    "                \"next_state\": {\"state\": state},\n",
    "                \"reward\": reward[0],\n",
    "                \"terminal\": terminal or step == max_steps\n",
    "            })\n",
    "    \n",
    "    \n",
    "    # update, update more if episode is longer, else less\n",
    "    for _ in range(step):\n",
    "        actor_loss, critic_loss = ddpg_per.update()\n",
    "    \n",
    "    \n",
    "    smoothed_total_reward = (smoothed_total_reward * 0.9 +\n",
    "                         total_reward * 0.1)\n",
    "    \n",
    "    line = f'{episode:<12}|' + '|'.join([f'{col:<12.4f}' for col in [smoothed_total_reward, critic_loss, actor_loss,]])\n",
    "    \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
